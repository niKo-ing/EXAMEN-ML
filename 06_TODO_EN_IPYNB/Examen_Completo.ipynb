{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Final: Predicción de Riesgo de Crédito (Home Credit)\n",
    "\n",
    "Este notebook consolida todo el flujo de trabajo del proyecto, desde la carga de datos hasta la evaluación del modelo, siguiendo la metodología CRISP-DM.\n",
    "\n",
    "## 1. Configuración e Importaciones\n",
    "Primero, importamos las librerías necesarias y configuramos el entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve, classification_report\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparación de Datos (Feature Engineering)\n",
    "\n",
    "En esta etapa, cargamos los datos crudos, realizamos limpieza, one-hot encoding y agregaciones. \n",
    "Por eficiencia, el código pesado se encuentra modularizado en `02_data_preparation/feature_engineering.py`. \n",
    "Aquí cargaremos directamente el dataset procesado `processed_data.parquet` si ya existe, o ejecutaremos una versión simplificada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al dataset procesado\n",
    "PROCESSED_DATA_PATH = '../processed_data.parquet'\n",
    "\n",
    "if os.path.exists(PROCESSED_DATA_PATH):\n",
    "    print(\"Cargando dataset procesado...\")\n",
    "    df = pd.read_parquet(PROCESSED_DATA_PATH)\n",
    "    print(f\"Dataset cargado: {df.shape}\")\n",
    "else:\n",
    "    print(\"El dataset procesado no existe. Por favor ejecute el script de feature engineering o asegúrese de que el archivo existe.\")\n",
    "    # Opcional: Llamar al script aquí si se desea\n",
    "    # !python 02_data_preparation/feature_engineering.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelado (LightGBM)\n",
    "\n",
    "Entrenamos un modelo LightGBM (Gradient Boosting Machine) optimizado para velocidad y rendimiento. Utilizamos `is_unbalance=True` para manejar la clase minoritaria (impagos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar datos de entrenamiento (donde TARGET no es nulo)\n",
    "train_df = df[df['TARGET'].notnull()]\n",
    "\n",
    "y = train_df['TARGET']\n",
    "X = train_df.drop(columns=['TARGET', 'SK_ID_CURR'])\n",
    "\n",
    "# Limpieza de nombres de columnas para LightGBM (sin caracteres especiales)\n",
    "X.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X.columns]\n",
    "\n",
    "print(f\"Dimensiones de entrenamiento: {X.shape}\")\n",
    "\n",
    "# Split Train/Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Dataset de LightGBM\n",
    "dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "dval = lgb.Dataset(X_val, label=y_val, reference=dtrain)\n",
    "\n",
    "# Hiperparámetros\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'is_unbalance': True,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.02,\n",
    "    'num_leaves': 31,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "print(\"Entrenando modelo...\")\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=1000,\n",
    "    valid_sets=[dtrain, dval],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(100)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluación del Modelo\n",
    "\n",
    "Analizamos el rendimiento del modelo utilizando AUC, Matriz de Confusión y Curva ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones\n",
    "y_pred_prob = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "y_pred_class = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# AUC Score\n",
    "auc = roc_auc_score(y_val, y_pred_prob)\n",
    "print(f\"AUC de Validación: {auc:.4f}\")\n",
    "\n",
    "# Matriz de Confusión\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_val, y_pred_class)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.ylabel('Etiqueta Real')\n",
    "plt.xlabel('Etiqueta Predicha')\n",
    "plt.show()\n",
    "\n",
    "# Curva ROC\n",
    "fpr, tpr, _ = roc_curve(y_val, y_pred_prob)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc:.4f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('Tasa de Falsos Positivos')\n",
    "plt.ylabel('Tasa de Verdaderos Positivos')\n",
    "plt.title('Curva ROC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importancia de Variables\n",
    "¿Qué características influyen más en la decisión del modelo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(model, max_num_features=20, importance_type='split', figsize=(10, 8))\n",
    "plt.title('Top 20 Variables Más Importantes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Simulación de API (Inferencia)\n",
    "\n",
    "Simulamos cómo funcionaría el endpoint `/evaluate_risk` con un cliente nuevo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_client(client_data, model, feature_names):\n",
    "    # Crear DataFrame y alinear columnas\n",
    "    input_df = pd.DataFrame([client_data])\n",
    "    \n",
    "    # Asegurar que todas las columnas del modelo existan (rellenar con 0 o NaN)\n",
    "    input_df = input_df.reindex(columns=feature_names)\n",
    "    \n",
    "    # Predecir\n",
    "    prob = model.predict(input_df)[0]\n",
    "    \n",
    "    decision = \"REVISIÓN MANUAL\"\n",
    "    if prob < 0.08: decision = \"APROBAR\"\n",
    "    elif prob > 0.3: decision = \"RECHAZAR\"\n",
    "        \n",
    "    return prob, decision\n",
    "\n",
    "# Tomar un ejemplo del set de validación\n",
    "sample_client = X_val.iloc[0].to_dict()\n",
    "prob, decision = evaluate_client(sample_client, model, X.columns.tolist())\n",
    "\n",
    "print(f\"Cliente Ejemplo:\")\n",
    "print(f\"Probabilidad de Impago: {prob:.2%}\")\n",
    "print(f\"Decisión Sugerida: {decision}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
